{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "# import urllib\n",
    "#DOWNLOAD_ROOT = \"Some filepath or url (github repository)\"\n",
    "#DATA_PATH = os.path.join(\"datasets\", \"your-dataset\")\n",
    "#DATA_URL = DOWNLOAD_ROOT + \"datasets/your-dataset/data.tgz\"\n",
    "\n",
    "#def fetch_data(data_url=DATA_URL, data_path=DATA_PATH):\n",
    "    #os.makedirs(data_path, exist_ok=True)\n",
    "    #tgz_path = os.path.join(data_path, \"data.tgz\")\n",
    "    #urllib.request.urlretrieve(data_url, tgz_path)\n",
    "    #data_tgz = tarfile.open(tgz_path)\n",
    "    #data_tgz.extractall(path=data_path)\n",
    "    #data_tgz.close()\n",
    "    \n",
    "\n",
    "#Load to Pandas DF\n",
    "#import pandas as pd\n",
    "\n",
    "#def load_data(data_path=DATA_PATH):\n",
    "    #csv_path = os.path.join(data_path, \"data.csv\")\n",
    "    #return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = load_data()\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#useful for getting quick description of data, total number of rows, each attribute's type, number of nonnull values\n",
    "#data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['variable'].value_counts()\n",
    "#Can be used to look at categorical sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the dataset at large\n",
    "#plots a histogram for each numerical feature\n",
    "#%matplotlib inline   # only in a Jupyter notebook\n",
    "#import matplotlib.pyplot as plt\n",
    "#data.hist(bins=50, figsize=(20,15))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to Watch Out for in Histograms:\n",
    "1. Make sure you understand how each value is being expressed/what the units of measurement are\n",
    "2. Are any values \"capped\"? (Abruptly stop at some value). You run the risk of the algorithm learning that prices never go above or below these arbitrary stops\n",
    "3. Attributes that have different scales will need feature scaling\n",
    "4. Tail-heavy histograms: may need transformations (log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing it at this time helps to avoid overfitting or picking the wrong architexture based on bias\n",
    "#function for test set creation:\n",
    "#import numpy as np\n",
    "\n",
    "#def split_train_test(data, test_ratio):\n",
    " #   shuffled_indices = np.random.permutation(len(data))           #shuffles the dataset\n",
    "  #  test_set_size = int(len(data) * test_ratio)                   #calculates test size based on ratio (often 20%)\n",
    "   # test_indices = shuffled_indices[:test_set_size]               #selects test set and from incdices\n",
    "   # train_indices = shuffled_indices[test_set_size:]              #assigns the rest to training\n",
    "   # return data.iloc[train_indices], data.iloc[test_indices]      #returns two different dfs for test and train\n",
    "    \n",
    "#Using the function\n",
    "#train_set, test_set = split_train_test(data, 0.2)\n",
    "#PROBLEM: this will give you a different dataset each time!\n",
    "#option: include np.random.seed(some_num) before the np.random_permutation line\n",
    "#This might break with new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a Train Test Split that Uses Changing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from zlib import crc32\n",
    "\n",
    "#def test_set_check(identifier, test_ratio):\n",
    "#    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\n",
    "\n",
    "#def split_train_test_by_id(data, test_ratio, id_column):\n",
    "#    ids = data[id_column]\n",
    "#    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n",
    "#    return data.loc[~in_test_set], data.loc[in_test_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#train_set, test_set = train_test_split (housing, test_size=.2, random_state=33)\n",
    "#Can pass multiple datasets to this and it will split on the same indices, useful if you have the data labels saved separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTANT FOR TEST SELECTION in a Smaller Sample\n",
    "#Need to make sure that important features have similar distributions in training and test!\n",
    "#Example: male and female, income distribution, etc.\n",
    "\n",
    "\n",
    "#Stratified Sampling Example:\n",
    "\n",
    "#from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "#split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "#for train_index, test_index in split.split(data, data[\"important_feature\"]):\n",
    "#    strat_train_set = data.loc[train_index]\n",
    "#    strat_test_set = data.loc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover and Visualize the Data to Gain Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corr_matrix = data.corr()      #this will help find linear correlations between attributes\n",
    "#from pandas.plotting import scatter_matrix()\n",
    "#attributes = [feature1,feature2,etc]\n",
    "#scatter_matrix(data[attributes], figsize=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with Attribute Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature engineering with the goal of creating features that are highly correlated with outcome variable (y)\n",
    "#doesn't have to be perfect, just need to gain some quick insights and then build a prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python37364bit0dddfd50435b402a8b45c517b8747bed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
